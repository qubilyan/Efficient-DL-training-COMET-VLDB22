{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving in Python with LeNet\n",
    "\n",
    "In this example, we'll explore learning with Caffe in Python, using the fully-exposed `Solver` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up the Python environment: we'll use the `pylab` import for numpy and plot inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import `caffe`, adding it to `sys.path` if needed. Make sure you've built pycaffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We'll be using the provided LeNet example data and networks (make sure you've downloaded the data and created the databases, as below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "Creating lmdb...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# run scripts from caffe root\n",
    "import os\n",
    "os.chdir(caffe_root)\n",
    "# Download data\n",
    "!data/mnist/get_mnist.sh\n",
    "# Prepare data\n",
    "!examples/mnist/create_mnist.sh\n",
    "# back to examples\n",
    "os.chdir('examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the net \n",
    "\n",
    "Now let's make a variant of LeNet, the classic 1989 convnet architecture.\n",
    "\n",
    "We'll need two external files to help out:\n",
    "* the net `prototxt`, defining the architecture and pointing to the train/test data\n",
    "* the solver `prototxt`, defining the learning parameters\n",
    "\n",
    "We start by creating the net. We'll write the net in a succinct and natural way as Python code that serializes to Caffe's protobuf model format.\n",
    "\n",
    "This network expects to read from pregenerated LMDBs, but reading directly from `ndarray`s is also possible using `MemoryDataLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet(lmdb, batch_size):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
    "                             transform_param=dict(scale=1./255), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('mnist/lenet_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('mnist/mnist_train_lmdb', 64)))\n",
    "    \n",
    "with open('mnist/lenet_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(lenet('mnist/mnist_test_lmdb', 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net has been written to disk in a more verbose but human-readable serialization format using Google's protobuf library. You can read, write, and modify this description directly. Let's take a look at the train net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer {\r\n",
      "  name: \"data\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00392156862745\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"mnist/mnist_train_lmdb\"\r\n",
      "    batch_size: 64\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 20\r\n",
      "    kernel_size: 5\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  convolution_param {\r\n",
      "    num_output: 50\r\n",
      "    kernel_size: 5\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"fc1\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"fc1\"\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 500\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"fc1\"\r\n",
      "  top: \"fc1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"score\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"fc1\"\r\n",
      "  top: \"score\"\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"score\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat mnist/lenet_auto_train.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the learning parameters, which are also written as a `prototxt` file (already provided on disk). We're using SGD with momentum, weight decay, and a specific learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The train/test net protocol buffer definition\r\n",
      "train_net: \"mnist/lenet_auto_train.prototxt\"\r\n",
      "test_net: \"mnist/lenet_auto_test.prototxt\"\r\n",
      "# test_iter specifies how many forward passes the test should carry out.\r\n",
      "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\r\n",
      "# covering the full 10,000 testing images.\r\n",
      "test_iter: 100\r\n",
      "# Carry out testing every 500 training iterations.\r\n",
      "test_interval: 500\r\n",
      "# The base learning rate, momentum and the weight decay of the network.\r\n",
      "base_lr: 0.01\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "# The learning rate policy\r\n",
      "lr_policy: \"inv\"\r\n",
      "gamma: 0.0001\r\n",
      "power: 0.75\r\n",
      "# Display every 100 iterations\r\n",
      "display: 100\r\n",
      "# The maximum number of iterations\r\n",
      "max_iter: 10000\r\n",
      "# snapshot intermediate results\r\n",
      "snapshot: 5000\r\n",
      "snapshot_prefix: \"mnist/lenet\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat mnist/lenet_auto_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading and checking the solver\n",
    "\n",
    "* Let's pick a device and load the solver. We'll use SGD (with momentum), but other methods (such as Adagrad and Nesterov's accelerated gradient) are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "### load the solver and create train and test nets\n",
    "solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "solver = caffe.SGDSolver('mnist/lenet_auto_solver.prototxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To get an idea of the architecture of our net, we can check the dimensions of the intermediate features (blobs) and parameters (these will also be useful to refer to when manipulating data later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', (64, 1, 28, 28)),\n",
       " ('label', (64,)),\n",
       " ('conv1', (64, 20, 24, 24)),\n",
       " ('pool1', (64, 20, 12, 12)),\n",
       " ('conv2', (64, 50, 8, 8)),\n",
       " ('pool2', (64, 50, 4, 4)),\n",
       " ('fc1', (64, 500)),\n",
       " ('score', (64, 10)),\n",
       " ('loss', ())]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1', (20, 1, 5, 5)),\n",
       " ('conv2', (50, 20, 5, 5)),\n",
       " ('fc1', (500, 800)),\n",
       " ('score', (10, 500))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before taking off, let's check that everything is loaded as we expect. We'll run a forward pass on the train and test nets and check that they contain our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': array(2.365971088409424, dtype=float32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.net.forward()  # train net\n",
    "solver.test_nets[0].forward()  # test net (there can be more than one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: [ 5.  0.  4.  1.  9.  2.  1.  3.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAABKCAYAAACfHW4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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